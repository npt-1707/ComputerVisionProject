# -*- coding: utf-8 -*-
"""ComputerVision_CLIP3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dgkY4BQrXa2-3QvbcVKRVKP3XB4p8Xsk

# DATA COLLECTION
"""

# !pip install transformers --upgrade
# !pip install datasets --upgrade
# !pip install accelerate --upgrade
# !pip install torchinfo

import datasets
import random
import torchvision.transforms.v2.functional as TF
import torchvision.transforms as T
import torch
# from datasets import Image
from PIL import Image

import numpy as np
from tqdm import tqdm
import torch
import torch.nn.functional as F
from transformers import CLIPModel, CLIPProcessor, AutoTokenizer, CLIPImageProcessor
import matplotlib.pyplot as plt
import tqdm
import json
import argparse
from sklearn.metrics import classification_report

parser = argparse.ArgumentParser(description='Medical GODEL based chatbot')
parser.add_argument('--config', type=str,default="config.json",
                    help='Config for all arguments')
args = parser.parse_args()

with open(args.config,"rb") as f:
  config = json.load(f)

def get_data(name = "cifar100", labeled_frag= 0.1,validation_frag = 0.1 , seed = 41 ):
  if name == "svhn":
    data = datasets.load_dataset(name,"cropped_digits")
    data_labeled =  datasets.concatenate_datasets([data["train"],data["extra"]])
    data_test = data["test"]
  elif name == "cifar100" :
    data = datasets.load_dataset(name)
    data_labeled = data["train"].rename_column('img','image').rename_column('fine_label','label')
    data_test = data["test"].rename_column('img','image').rename_column('fine_label','label')
  elif name == "cifar10":
    data = datasets.load_dataset(name)
    data_labeled = data["train"].rename_column('img','image')
    data_test = data["test"].rename_column('img','image')
  else:
    raise Exception("Dataset not in our concern")
  data_labeled = data_labeled.train_test_split(test_size = validation_frag, shuffle = True, stratify_by_column='label', seed = seed)
  data_validation = data_labeled["test"]
  data_train = data_labeled["train"]
  data_train_full = data_train.train_test_split(test_size = labeled_frag, shuffle = True, stratify_by_column='label', seed = seed)
  data_train_labeled = data_train_full["test"]
  data_train_unlabeled = data_train_full["train"]
  data_train_unlabeled = data_train_unlabeled.map( lambda _: {"label": -1}, features=data_train_unlabeled.features)
  return {"train_labeled": data_train_labeled,
          "train_unlabeled": data_train_unlabeled,
          "validation":data_validation,
          "test": data_test}

dataset = get_data(name = config["dataset_name"] ,labeled_frag = config["dataset_label_frag"], seed = config["dataset_seed"], validation_frag = config["dataset_validation_frag"])
train_labeled_dataset = dataset["train_labeled"]
train_unlabeled_dataset = dataset["train_unlabeled"]
validation_dataset = dataset["validation"]
test_dataset = dataset["test"]

print(train_labeled_dataset, train_unlabeled_dataset, validation_dataset, test_dataset)

"""# Data Augmentation

## Method
Image augmentation
- Random cropping
- Random resizing
- Random color jitter
- Random vertical or horizontal flipping
- Random rotation
- Random blur
"""


def image_augmentation(dataset):
  images = []
  for t,i in enumerate(dataset["image"]):
    k = random.choice(["grayscale","random resizing cropping","random color jitter","random flipping","random rotation","blur"])
    if k == "grayscale":
      images.append(T.v2.Resize(size = (224,224))(T.v2.Grayscale(num_output_channels=3)(i)))
    if k == "random resizing cropping":
      images.append(T.v2.RandomResizedCrop(size = 224, scale = (0.5,1.0))(i))
    if k == "random color jitter":
      images.append(T.v2.Resize(size = (224,224))(T.v2.ColorJitter()(i)))
    if k == "random flipping":
      n = random.choice(["vertical","horizontal"])
      if n == "vertical":
        images.append(T.v2.Resize(size = (224,224))(TF.vflip(i)))
      if n == "horizontal":
        images.append(T.v2.Resize(size = (224,224))(TF.hflip(i)))
    if k == "random rotation":
      images.append(T.v2.Resize(size = (224,224))(T.v2.RandomRotation(degrees = (0,180))(i)))
    if k == "blur":
      images.append(T.v2.Resize(size = (224,224))(TF.gaussian_blur(i, (3,3))))
  dataset["image"] = images
  return dataset

def resize(examples):
  examples["image"] = [T.v2.Resize(size = (224,224))(i) for i in examples["image"]]
  return examples

"""# Model"""


"""## Iter1

### Data Augmentation
"""

augment_train_labeled_dataset_1 = train_labeled_dataset.map(image_augmentation, batched = True, load_from_cache_file =  False)
augment_train_labeled_dataset_2 = train_labeled_dataset.map(image_augmentation, batched = True, load_from_cache_file =  False)
train_labeled_dataset = train_labeled_dataset.map(resize, batched = True, load_from_cache_file =  False)
train_unlabeled_dataset = train_unlabeled_dataset.map(resize, batched = True, load_from_cache_file =  False)
validation_dataset = validation_dataset.map(resize, batched = True, load_from_cache_file =  False)
test_dataset = test_dataset.map(resize, batched = True, load_from_cache_file =  False)

new_train_labeled_dataset= datasets.concatenate_datasets([train_labeled_dataset, augment_train_labeled_dataset_1,augment_train_labeled_dataset_2])

"""### Model and Trainer"""

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
tokenizer = AutoTokenizer.from_pretrained("openai/clip-vit-base-patch32")
image_processor = CLIPImageProcessor.from_pretrained("openai/clip-vit-base-patch32")

class ClipModel(torch.nn.Module):
  def __init__(self, model, device, img_processor):
    super(ClipModel,self).__init__()
    self.baseline = model
    self.device = device
    self.img_processor = img_processor
  def forward(self,image, positive, negative):
    image = self.img_processor(image, return_tensors = "pt")["pixel_values"]
    positive_logits = self.baseline.forward(pixel_values = image.to(self.device),input_ids = positive["input_ids"].to(self.device), attention_mask =positive["attention_mask"].to(self.device)).logits_per_image.softmax(dim=1)
    negative_logits = self.baseline.forward(pixel_values = image.to(self.device),input_ids = negative["input_ids"].to(self.device), attention_mask =negative["attention_mask"].to(self.device)).logits_per_image.softmax(dim=1)
    return positive_logits, negative_logits
  def predict(self, image):
    self.baseline.eval()
    labels = [f"a photo of a {i}" for i in new_train_labeled_dataset.features['label'].names]
    image = self.img_processor(image,return_tensors = "pt")["pixel_values"]
    label_tok = tokenizer(labels, return_tensors="pt", padding=True)
    outputs = self.baseline(pixel_values = image.to(self.device),input_ids = label_tok["input_ids"].to(self.device), attention_mask =label_tok["attention_mask"].to(self.device))
    logits_per_image = outputs.logits_per_image
    # print(logits_per_image)
    pred = torch.argmax(logits_per_image,dim = -1)
    return pred, torch.max(logits_per_image)
  def predict_all(self, images):
    # print(images)
    preds = [self.predict(i)[0] for i in images]
    return torch.stack(preds)

class TripletLoss(torch.nn.Module):
  def __init__(self, margin):
    super(TripletLoss,self).__init__()
    self.margin = margin
  def forward(self, pos_dis, neg_dis, size_avg = True):
    losses = torch.nn.functional.relu(-pos_dis + neg_dis + self.margin)
    return losses.mean() if size_avg else losses.sum()

def collate(*batch):
  minibatch = batch[0]
  image = [i["image"] for i in minibatch]
  positive = ["A photo of a "+new_train_labeled_dataset.features['label'].int2str(i["label"].item()) for i in minibatch]
  negative = ["A photo of a "+random.choice([j for j in new_train_labeled_dataset.features['label'].names if j != i]) for i in positive]
  label = [i["label"] for i in minibatch]
  return torch.stack(image).permute(0,3,1,2),tokenizer(positive, return_tensors="pt", padding=True), tokenizer(negative, return_tensors="pt", padding=True),torch.tensor(label)

def accuracy(pred, label):
  acc = 1 - (torch.count_nonzero(pred - label)/len(pred)).item()
  return acc

epoch = config["model_epoch"]
learning_rate = config["optimizer_learning_rate"]
weight_decay = config["optimizer_weight_decay"]
batch_size = config["model_batch_size"]
margin = config["loss_margin"]
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

train_labeled_dataloader = torch.utils.data.DataLoader(new_train_labeled_dataset.with_format("torch"), batch_size=batch_size, shuffle=True, collate_fn = collate)
train_unlabeled_dataloader = torch.utils.data.DataLoader(train_unlabeled_dataset.with_format("torch"), batch_size=batch_size, shuffle=True, collate_fn = collate)
validation_dataloader = torch.utils.data.DataLoader(validation_dataset.with_format("torch"), batch_size=batch_size, shuffle=True, collate_fn = collate)
test_dataloader = torch.utils.data.DataLoader(test_dataset.with_format("torch"), batch_size=batch_size, shuffle=True, collate_fn = collate)

clip = ClipModel(model,device, image_processor).to(device)
optimizer = torch.optim.AdamW(clip.parameters(), lr=learning_rate, weight_decay=weight_decay)
criterion = TripletLoss(margin= margin).to(device)

for ep in range(epoch):
  train_loss = 0
  valid_loss = 0
  valid_acc = 0
  clip.train()
  for i,batch in tqdm.tqdm(enumerate(train_labeled_dataloader),total = len(train_labeled_dataloader)):
    image, positive, negative,label = batch
    pos_logits, neg_logits = clip(image.to(device), positive.to(device), negative.to(device))
    optimizer.zero_grad()
    loss = criterion(pos_logits, neg_logits)
    train_loss += loss
    loss.backward()
    optimizer.step()
  clip.eval()
  with torch.no_grad():
    for i,batch in tqdm.tqdm(enumerate(validation_dataloader),total = len(validation_dataloader)):
      image, positive, negative, label = batch
      pos_logits, neg_logits = clip(image.to(device), positive.to(device), negative.to(device))
      loss = criterion(pos_logits, neg_logits)
      valid_loss += loss
      valid_acc += accuracy(clip.predict_all(image.to(device)).squeeze(), label.to(device))
  print("\n Epoch "+str(ep)+", Training Loss: "+str(train_loss.item()/len(train_labeled_dataloader))\
            +", Validation Loss: " + str(valid_loss.item()/len(validation_dataloader))+ ",Validation Accuracy: "+ str(valid_acc/len(validation_dataloader)))
  with open(config["logging_file"],"w") as f:
    f.write("Epoch "+str(ep)+", Training Loss: "+str(train_loss.item()/len(train_labeled_dataloader))\
            +", Validation Loss: " + str(valid_loss.item()/len(validation_dataloader))+ ",Validation Accuracy: "+ str(valid_acc/len(validation_dataloader)))
  torch.save(clip.state_dict(), config["checkpoint"])
#     img, positive, negative = batch

"""## Pseudo Label"""

train_unlabeled_dataset = train_unlabeled_dataset.map(resize, batched = True, load_from_cache_file =  False)

thres = config["pseudo_thres"]
images = []
labels = []
for i in train_unlabeled_dataset["image"]:
  if clip.predict(i)[1] > thres:
    images.append(i)
    labels.appen(clip.predict(i)[0].item())
pseudo_label_dataset = datasets.Dataset.from_dict({"image":images, "label":labels})

"""## Iter 2

### Data Augmentation
"""

augment_train_pseudo_dataset = pseudo_label_dataset.map(image_augmentation, batched = True, load_from_cache_file =  False)
new_train_pseudo_dataset_1= datasets.concatenate_datasets([new_train_labeled_dataset,pseudo_label_dataset, augment_train_pseudo_dataset])

"""### Training"""

new_train_pseudo_dataloader = torch.utils.data.DataLoader(new_train_pseudo_dataset_1.with_format("torch"), batch_size=batch_size, shuffle=True, collate_fn = collate)
optimizer = torch.optim.AdamW(clip.parameters(), lr=learning_rate, weight_decay=weight_decay)
criterion = TripletLoss(margin= margin).to(device)
for ep in range(epoch):
  train_loss = 0
  valid_loss = 0
  valid_acc = 0
  clip.train()
  for i,batch in tqdm.tqdm(enumerate(new_train_pseudo_dataloader),total = len(new_train_pseudo_dataloader)):
    image, positive, negative,label = batch
    pos_logits, neg_logits = clip(image.to(device), positive.to(device), negative.to(device))
    optimizer.zero_grad()
    loss = criterion(pos_logits, neg_logits)
    train_loss += loss
    loss.backward()
    optimizer.step()
  clip.eval()
  with torch.no_grad():
    for i,batch in tqdm.tqdm(enumerate(validation_dataloader),total = len(validation_dataloader)):
      image, positive, negative, label = batch
      pos_logits, neg_logits = clip(image.to(device), positive.to(device), negative.to(device))
      loss = criterion(pos_logits, neg_logits)
      valid_loss += loss
      valid_acc += accuracy(clip.predict_all(image.to(device)).squeeze(), label.to(device))
  print("\n Epoch "+str(ep)+", Training Loss: "+str(train_loss.item()/len(train_labeled_dataloader))\
            +", Validation Loss: " + str(valid_loss.item()/len(validation_dataloader))+ ",Validation Accuracy: "+ str(valid_acc/len(validation_dataloader)))
  with open(config["logging_file_pseudo"],"w") as f:
    f.write("Epoch "+str(ep)+", Training Loss: "+str(train_loss.item()/len(train_labeled_dataloader))\
            +", Validation Loss: " + str(valid_loss.item()/len(validation_dataloader))+ ",Validation Accuracy: "+ str(valid_acc/len(validation_dataloader)))
  torch.save_state_dict(clip.state_dict(),config["model_path"])

"""### Evaluating"""

#clip = ClipModel(model,device, image_processor).to(device)
#clip.load_state_dict(torch.load("checkpoint_final.pt"))
label = test_dataset["label"]
predictions = [clip.predict(i)[0].item() for i in test_dataset["image"]]


classification_report(np.array(predictions),np.array(label), labels = test_dataset.features["label"].names )