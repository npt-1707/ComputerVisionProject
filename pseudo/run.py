# -*- coding: utf-8 -*-
"""ComputerVision_CLIP3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dgkY4BQrXa2-3QvbcVKRVKP3XB4p8Xsk

# DATA COLLECTION
"""

# !pip install transformers --upgrade
# !pip install datasets --upgrade
# !pip install accelerate --upgrade
# !pip install torchinfo

import datasets
import random
import torchvision.transforms.v2.functional as TF
import torchvision.transforms as T
import torch
# from datasets import Image
from PIL import Image
from datasets import ClassLabel
import numpy as np
from tqdm import tqdm
import torch
import torch.nn.functional as F
from transformers import CLIPModel, CLIPProcessor, AutoTokenizer, CLIPImageProcessor
import matplotlib.pyplot as plt
import tqdm
import json
import argparse
from sklearn.metrics import classification_report

parser = argparse.ArgumentParser(description='Medical GODEL based chatbot')
parser.add_argument('--config', type=str,default="config.json",
                    help='Config for all arguments')
args = parser.parse_args()

with open(args.config,"rb") as f:
  config = json.load(f)

def get_data(name = "cifar100", labeled_frag= 0.1,validation_frag = 0.1 , seed = 41 ):
  if name == "svhn":
    data = datasets.load_dataset(name,"cropped_digits")
    data_labeled =  datasets.concatenate_datasets([data["train"],data["extra"]])
    data_test = data["test"]
  elif name == "cifar100" :
    data = datasets.load_dataset(name)
    data_labeled = data["train"].rename_column('img','image').rename_column('fine_label','label')
    data_test = data["test"].rename_column('img','image').rename_column('fine_label','label')
  elif name == "cifar10":
    data = datasets.load_dataset(name)
    data_labeled = data["train"].rename_column('img','image')
    data_test = data["test"].rename_column('img','image')
  else:
    raise Exception("Dataset not in our concern")
  data_labeled = data_labeled.train_test_split(test_size = validation_frag, shuffle = True, stratify_by_column='label', seed = seed)
  data_validation = data_labeled["test"]
  data_train = data_labeled["train"]
  data_train_full = data_train.train_test_split(test_size = labeled_frag, shuffle = True, stratify_by_column='label', seed = seed)
  data_train_labeled = data_train_full["test"]
  data_train_unlabeled = data_train_full["train"]
  data_train_unlabeled = data_train_unlabeled.map( lambda _: {"label": -1}, features=data_train_unlabeled.features)
  return {"train_labeled": data_train_labeled,
          "train_unlabeled": data_train_unlabeled,
          "validation":data_validation,
          "test": data_test}

dataset = get_data(name = config["dataset_name"] ,labeled_frag = config["dataset_label_frag"], seed = config["dataset_seed"], validation_frag = config["dataset_validation_frag"])
train_labeled_dataset = dataset["train_labeled"]
train_unlabeled_dataset = dataset["train_unlabeled"]
validation_dataset = dataset["validation"]
test_dataset = dataset["test"]

print(train_labeled_dataset, train_unlabeled_dataset, validation_dataset, test_dataset)

"""# Data Augmentation

## Method
Image augmentation
- Random cropping
- Random resizing
- Random color jitter
- Random vertical or horizontal flipping
- Random rotation
- Random blur
"""


def image_augmentation(dataset):
  images = []
  for t,i in enumerate(dataset["image"]):
    k = random.choice(["grayscale","random resizing cropping","random color jitter","random flipping","random rotation","blur"])
    if k == "grayscale":
      images.append(T.v2.Resize(size = (224,224))(T.v2.Grayscale(num_output_channels=3)(i)))
    if k == "random resizing cropping":
      images.append(T.v2.RandomResizedCrop(size = 224, scale = (0.5,1.0))(i))
    if k == "random color jitter":
      images.append(T.v2.Resize(size = (224,224))(T.v2.ColorJitter()(i)))
    if k == "random flipping":
      n = random.choice(["vertical","horizontal"])
      if n == "vertical":
        images.append(T.v2.Resize(size = (224,224))(TF.vflip(i)))
      if n == "horizontal":
        images.append(T.v2.Resize(size = (224,224))(TF.hflip(i)))
    if k == "random rotation":
      images.append(T.v2.Resize(size = (224,224))(T.v2.RandomRotation(degrees = (0,180))(i)))
    if k == "blur":
      images.append(T.v2.Resize(size = (224,224))(TF.gaussian_blur(i, (3,3))))
  dataset["image"] = images
  return dataset

def resize(examples):
  examples["image"] = [T.v2.Resize(size = (224,224))(i) for i in examples["image"]]
  return examples

"""# Model"""


"""## Iter1

### Data Augmentation
"""

augment_train_labeled_dataset_1 = train_labeled_dataset.map(image_augmentation, batched = True, load_from_cache_file =  False)
augment_train_labeled_dataset_2 = train_labeled_dataset.map(image_augmentation, batched = True, load_from_cache_file =  False)
train_labeled_dataset = train_labeled_dataset.map(resize, batched = True, load_from_cache_file =  False)
train_unlabeled_dataset = train_unlabeled_dataset.map(resize, batched = True, load_from_cache_file =  False)
validation_dataset = validation_dataset.map(resize, batched = True, load_from_cache_file =  False)
test_dataset = test_dataset.map(resize, batched = True, load_from_cache_file =  False)

new_train_labeled_dataset= datasets.concatenate_datasets([train_labeled_dataset, augment_train_labeled_dataset_1,augment_train_labeled_dataset_2])

"""### Model and Trainer"""

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
tokenizer = AutoTokenizer.from_pretrained("openai/clip-vit-base-patch32")
image_processor = CLIPImageProcessor.from_pretrained("openai/clip-vit-base-patch32")

class ClipModel(torch.nn.Module):
  def __init__(self, model, device, img_processor):
    super(ClipModel,self).__init__()
    self.baseline = model
    self.device = device
    self.label =  tokenizer([f"a photo of a {i}" for i in test_dataset.features['label'].names], return_tensors="pt", padding=True)
    self.img_processor = img_processor
  def forward(self,image, positive, negative):
    image = self.img_processor(image, return_tensors = "pt")["pixel_values"]
    image_embedd = self.baseline.get_image_features(pixel_values =image.to(self.device))
    positive_embedd = self.baseline.get_text_features(input_ids = positive["input_ids"].to(self.device), attention_mask =positive["attention_mask"].to(self.device))
    negative_embedd = self.baseline.get_text_features(input_ids = negative["input_ids"].to(self.device), attention_mask =negative["attention_mask"].to(self.device))
    positive_logits = torch.nn.functional.cosine_similarity(image_embedd,positive_embedd,dim = 1)
    negative_logits = torch.nn.functional.cosine_similarity(image_embedd, negative_embedd,dim = 1)
    label_logits = self.baseline.forward(pixel_values = image.to(self.device),input_ids = self.label["input_ids"].to(self.device), attention_mask =self.label["attention_mask"].to(self.device)).logits_per_image
    # positive_logits = torch.diagonal(self.baseline.forward(pixel_values = image.to(self.device),input_ids = positive["input_ids"].to(self.device), attention_mask =positive["attention_mask"].to(self.device)).logits_per_image,0,0,1)
    # negative_logits = torch.diagonal(self.baseline.forward(pixel_values = image.to(self.device),input_ids = negative["input_ids"].to(self.device), attention_mask =negative["attention_mask"].to(self.device)).logits_per_image,0,0,1)
    # print("Positive: ",positive_logits)
    # print("Negative: ",negative_logits)
    return positive_logits, negative_logits, label_logits, image_embedd
  def predict(self, image):
    self.baseline.eval()
    image = self.img_processor(image,return_tensors = "pt")["pixel_values"]
    outputs = self.baseline(pixel_values = image.to(self.device),input_ids = self.label["input_ids"].to(self.device), attention_mask =self.label["attention_mask"].to(self.device))
    logits_per_image = outputs.logits_per_image
    pred = torch.argmax(logits_per_image,dim = -1)
    return pred, torch.max(logits_per_image.softmax(dim=1))
  def predict_all(self, images):
    self.baseline.eval()
    images = self.img_processor(images,return_tensors = "pt")["pixel_values"]
    outputs = self.baseline(pixel_values = images.to(self.device),input_ids = self.label["input_ids"].to(self.device), attention_mask =self.label["attention_mask"].to(self.device))
    logits_per_image = outputs.logits_per_image.softmax(dim=1)
    preds = torch.argmax(logits_per_image,dim = -1)
    return preds

def collate(*batch):
  minibatch = batch[0]
  image = [i["image"] for i in minibatch]
  positive = ["A photo of a "+new_train_labeled_dataset.features['label'].int2str(i["label"].item()) for i in minibatch]
  negative = ["A photo of a "+random.choice([j for j in new_train_labeled_dataset.features['label'].names if j != i]) for i in positive]
  label = [i["label"] for i in minibatch]
  return torch.stack(image).permute(0,3,1,2),tokenizer(positive, return_tensors="pt", padding=True), tokenizer(negative, return_tensors="pt", padding=True),torch.tensor(label)

def accuracy(pred, label):
  acc = 1 - (torch.count_nonzero(pred - label)/len(pred)).item()
  return acc

epoch = config["model_epoch"]
learning_rate = config["optimizer_learning_rate"]
weight_decay = config["optimizer_weight_decay"]
batch_size = config["model_batch_size"]
margin = config["loss_margin"]
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

train_labeled_dataloader = torch.utils.data.DataLoader(new_train_labeled_dataset.with_format("torch"), batch_size=batch_size, shuffle=True, collate_fn = collate)
train_unlabeled_dataloader = torch.utils.data.DataLoader(train_unlabeled_dataset.with_format("torch"), batch_size=batch_size, shuffle=True, collate_fn = collate)
validation_dataloader = torch.utils.data.DataLoader(validation_dataset.with_format("torch"), batch_size=batch_size, shuffle=True, collate_fn = collate)
test_dataloader = torch.utils.data.DataLoader(test_dataset.with_format("torch"), batch_size=batch_size, shuffle=True, collate_fn = collate)

clip = ClipModel(model,device, image_processor).to(device)
optimizer = torch.optim.AdamW(clip.parameters(), lr=learning_rate, weight_decay=weight_decay)
criterion2 = torch.nn.CrossEntropyLoss().to(device)

count = 0
minl = 2
for ep in range(epoch):
  train_loss = 0
  valid_loss = 0
  valid_acc = 0
  clip.train()
  for i,batch in tqdm.tqdm(enumerate(train_labeled_dataloader),total = len(train_labeled_dataloader)):
    image, positive, negative,label = batch
    pos_logits, neg_logits,lab_logits,image_embedd = clip(image.to(device), positive.to(device), negative.to(device))
    optimizer.zero_grad()
    # loss = criterion(pos_logits, neg_logits)
    loss = criterion2(lab_logits, label.to(device))
    # loss = criterion3(image_embedd.unsqueeze(1), label)
    train_loss += loss
    loss.backward()
    optimizer.step()
  clip.eval()
  with torch.no_grad():
    for i,batch in tqdm.tqdm(enumerate(validation_dataloader),total = len(validation_dataloader)):
      image1, positive1, negative1, label1 = batch
      pos_logits1, neg_logits1,lab_logits1,image_embedd1 = clip(image1.to(device), positive1.to(device), negative1.to(device))
      # print(pos_logits, neg_logits)
      # loss = criterion(pos_logits, neg_logits)
      loss = criterion2(lab_logits1, label1.to(device))
      # loss = criterion3(image_embedd1.unsqueeze(1), label1)
      valid_loss += loss
      valid_acc += accuracy(clip.predict_all(image1.to(device)).squeeze(), label1.to(device))
      # print(accuracy(clip.predict_all(image.to(device)).squeeze(), label.to(device)))
      # break
  if valid_loss.item()/len(validation_dataloader) < minl:
    minl = valid_loss.item()/len(validation_dataloader)
  if valid_loss.item()/len(validation_dataloader) > minl:
    count+=1
    if count > 5:
        break
  print("\n Epoch "+str(ep)+", Training Loss: "+str(train_loss.item()/len(train_labeled_dataloader))\
            +", Validation Loss: " + str(valid_loss.item()/len(validation_dataloader))+ ",Validation Accuracy: "+ str(valid_acc/len(validation_dataloader)))
  with open(config["logging_file"],"a") as f:
    f.write("Epoch "+str(ep)+", Training Loss: "+str(train_loss.item()/len(train_labeled_dataloader))\
            +", Validation Loss: " + str(valid_loss.item()/len(validation_dataloader))+ ",Validation Accuracy: "+ str(valid_acc/len(validation_dataloader)))
  torch.save(clip.state_dict(), config["checkpoint"])
#     img, positive, negative = batch

"""## Pseudo Label"""

train_unlabeled_dataset = train_unlabeled_dataset.map(resize, batched = True, load_from_cache_file =  False)

thres = config["pseudo_thres"]
images = []
labels = []
train_unlabeled_dataset = dataset["train_unlabeled"]
unlabel_images = list(train_unlabeled_dataset["image"])
for i in tqdm.tqdm(unlabel_images,total = len(unlabel_images)):
  output = clip.predict(i)
  if output[1].item() > thres:
    images.append(i)
    labels.append(output[0].item())
pseudo_label_dataset = datasets.Dataset.from_dict({"image":images, "label":labels})
pseudo_label_dataset = pseudo_label_dataset.cast_column("label",ClassLabel(num_classes = len(test_dataset.features['label'].names),names=test_dataset.features['label'].names))

"""## Iter 2

### Data Augmentation
"""

augment_train_pseudo_dataset = pseudo_label_dataset.map(image_augmentation, batched = True, load_from_cache_file =  False)
pseudo_label_dataset = pseudo_label_dataset.map(resize, batched = True, load_from_cache_file =  False)
new_train_pseudo_dataset_1= datasets.concatenate_datasets([new_train_labeled_dataset,pseudo_label_dataset, augment_train_pseudo_dataset])

"""### Training"""

learning_rate = config["learning_rate_p2"]
weight_decay = config["weight_decay_p2"]
epoch = config["epoch_p2"]
batch_size = config["batch_size_p2"]
margin = config["margin_p2"]
new_train_pseudo_dataloader = torch.utils.data.DataLoader(new_train_pseudo_dataset_1.with_format("torch"), batch_size=batch_size, shuffle=True, collate_fn = collate)
validation_dataloader = torch.utils.data.DataLoader(validation_dataset.with_format("torch"), batch_size=batch_size, shuffle=True, collate_fn = collate)
optimizer1 = torch.optim.AdamW(clip.parameters(), lr=learning_rate, weight_decay=weight_decay)
# criterion = TripletLoss(margin= margin).to(device)
criterion2 = torch.nn.CrossEntropyLoss()
count1 = 0
minl1 = 2
for ep in range(epoch):
  train_loss = 0
  valid_loss = 0
  valid_acc = 0
  clip.train()
  for i,batch in tqdm.tqdm(enumerate(new_train_pseudo_dataloader),total = len(new_train_pseudo_dataloader)):
    image, positive, negative,label = batch
    pos_logits, neg_logits,lab_logits,image_embedd = clip(image.to(device), positive.to(device), negative.to(device))
    optimizer1.zero_grad()
    # loss = criterion(pos_logits, neg_logits)
    loss = criterion2(lab_logits, label.to(device))
    train_loss += loss
    loss.backward()
    optimizer1.step()
  clip.eval()
  with torch.no_grad():
    for i,batch in tqdm.tqdm(enumerate(validation_dataloader),total = len(validation_dataloader)):
      image1, positive1, negative1, label1 = batch
      pos_logits, neg_logits,lab_logits,image_embedd = clip(image1.to(device), positive1.to(device), negative1.to(device))
      # loss = criterion(pos_logits, neg_logits)
      loss = criterion2(lab_logits, label1.to(device))
      valid_loss += loss
      valid_acc += accuracy(clip.predict_all(image1.to(device)).squeeze(), label1.to(device))
  if valid_loss.item()/len(validation_dataloader) < minl1:
    minl = valid_loss.item()/len(validation_dataloader)
  if valid_loss.item()/len(validation_dataloader) > minl1:
    count1+=1
    if count1 > 3:
        break
  print("\n Epoch "+str(ep)+", Training Loss: "+str(train_loss.item()/len(train_labeled_dataloader))\
            +", Validation Loss: " + str(valid_loss.item()/len(validation_dataloader))+ ",Validation Accuracy: "+ str(valid_acc/len(validation_dataloader)))
  with open(config["logging_file_pseudo"],"a") as f:
    f.write("Epoch "+str(ep)+", Training Loss: "+str(train_loss.item()/len(train_labeled_dataloader))\
            +", Validation Loss: " + str(valid_loss.item()/len(validation_dataloader))+ ",Validation Accuracy: "+ str(valid_acc/len(validation_dataloader)))
  torch.save_state_dict(clip.state_dict(),config["model_path"])

"""### Evaluating"""

#clip = ClipModel(model,device, image_processor).to(device)
#clip.load_state_dict(torch.load("checkpoint_final.pt"))
label = test_dataset["label"]
predictions = [clip.predict(i)[0].item() for i in tqdm.tqdm(test_dataset["image"],total = len(test_dataset["image"]))]
with open(config["results"], "a") as f:
    f.writelines([str(i[0])+' , '+str(i[1]) for i in list(zip(label,predictions))])
result = classification_report(np.array(predictions),np.array(label), output_dict=True)
with open(config["classification_report"],"w") as f:
  json.dump(result, f)